# Proposition : Syst√®me de Queue pour le Scraping

## üìã Probl√®me Actuel

### Sympt√¥mes
- Le bouton "Scrape New Data" fait planter la page apr√®s quelques minutes
- Les requ√™tes HTTP synchrones bloquent le navigateur
- 10 scrapers lanc√©s en parall√®le √ó plusieurs keywords = beaucoup de requ√™tes simultan√©es
- Pas de moyen de suivre la progression en temps r√©el
- Pas de possibilit√© d'annuler toutes les t√¢ches facilement

### Causes Identifi√©es
1. **Frontend** : Tous les scrapers sont lanc√©s en parall√®le avec `Promise.all()` ou des promesses parall√®les
2. **Backend** : Les endpoints `/scrape/*` sont synchrones et peuvent prendre plusieurs minutes chacun
3. **Pas de timeout** : Les requ√™tes peuvent rester bloqu√©es ind√©finiment
4. **Pas de queue** : Toutes les t√¢ches sont ex√©cut√©es simultan√©ment sans limite

## üéØ Objectifs

1. **D√©coupler le frontend du backend** : Le scraping doit tourner en arri√®re-plan
2. **Syst√®me de queue** : G√©rer les t√¢ches de mani√®re asynchrone avec une file d'attente
3. **Monitoring en temps r√©el** : Afficher la progression dans les logs et dans l'UI
4. **Annulation globale** : Bouton "Cancel All" pour arr√™ter toutes les t√¢ches en cours
5. **Persistance** : Les jobs doivent survivre aux red√©marrages du serveur
6. **Logs d√©taill√©s** : Chaque √©tape doit √™tre logg√©e pour le debugging

## üèóÔ∏è Architecture Propos√©e

### 1. Backend : Syst√®me de Queue avec Celery ou ThreadPoolExecutor

#### Option A : Celery + Redis/RabbitMQ (Recommand√© pour production)
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Frontend  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  FastAPI ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Celery    ‚îÇ
‚îÇ             ‚îÇ     ‚îÇ  (API)   ‚îÇ     ‚îÇ   Worker    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ                 ‚îÇ
                           ‚îÇ                 ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   Redis/    ‚îÇ   ‚îÇ  Scrapers   ‚îÇ
                    ‚îÇ  RabbitMQ   ‚îÇ   ‚îÇ  (Tasks)    ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Avantages :**
- ‚úÖ Scalable (plusieurs workers)
- ‚úÖ Persistance des jobs
- ‚úÖ Retry automatique
- ‚úÖ Monitoring avec Flower

**Inconv√©nients :**
- ‚ùå D√©pendance externe (Redis/RabbitMQ)
- ‚ùå Plus complexe √† d√©ployer

#### Option B : ThreadPoolExecutor + Base de donn√©es (Plus simple)
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Frontend  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  FastAPI ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ ThreadPool   ‚îÇ
‚îÇ             ‚îÇ     ‚îÇ  (API)   ‚îÇ     ‚îÇ  Executor    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ                 ‚îÇ
                           ‚îÇ                 ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   SQLite    ‚îÇ   ‚îÇ  Scrapers   ‚îÇ
                    ‚îÇ  (Jobs DB)  ‚îÇ   ‚îÇ  (Tasks)    ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Avantages :**
- ‚úÖ Pas de d√©pendance externe
- ‚úÖ Simple √† d√©ployer
- ‚úÖ Utilise d√©j√† SQLite

**Inconv√©nients :**
- ‚ùå Moins scalable
- ‚ùå Pas de retry automatique

**Recommandation : Option B** pour commencer (d√©j√† un syst√®me de jobs dans `main.py`)

### 2. Structure des Jobs

```python
{
    "job_id": "uuid",
    "status": "pending|running|completed|failed|cancelled",
    "created_at": "timestamp",
    "started_at": "timestamp",
    "completed_at": "timestamp",
    "progress": {
        "total": 30,  # keywords √ó sources
        "completed": 5,
        "current_task": "Scraping X/Twitter for 'ovh vps'"
    },
    "results": [
        {"source": "x", "keyword": "ovh vps", "added": 10},
        {"source": "github", "keyword": "ovh vps", "added": 5}
    ],
    "errors": [],
    "logs": [
        {"timestamp": "...", "level": "info", "message": "..."}
    ]
}
```

### 3. Endpoints API

#### Cr√©er un job de scraping
```http
POST /api/scrape/jobs
Content-Type: application/json

{
    "keywords": ["ovh vps", "ovh hosting"],
    "sources": ["x", "github", "stackoverflow", "news", "reddit", "trustpilot", "ovh-forum", "mastodon", "g2-crowd", "linkedin"],
    "limit": 50,
    "concurrency": 2  # Nombre de scrapers en parall√®le
}

Response:
{
    "job_id": "uuid",
    "status": "pending",
    "message": "Job cr√©√© avec succ√®s"
}
```

#### Obtenir le statut d'un job
```http
GET /api/scrape/jobs/{job_id}

Response:
{
    "job_id": "uuid",
    "status": "running",
    "progress": {
        "total": 20,
        "completed": 8,
        "current_task": "Scraping X/Twitter for 'ovh vps'"
    },
    "results": [...],
    "logs": [...]
}
```

#### Annuler un job
```http
POST /api/scrape/jobs/{job_id}/cancel

Response:
{
    "job_id": "uuid",
    "status": "cancelled",
    "message": "Job annul√© avec succ√®s"
}
```

#### Annuler tous les jobs
```http
POST /api/scrape/jobs/cancel-all

Response:
{
    "cancelled_count": 3,
    "message": "3 jobs annul√©s"
}
```

#### Lister tous les jobs
```http
GET /api/scrape/jobs?status=running&limit=10

Response:
{
    "jobs": [
        {
            "job_id": "uuid",
            "status": "running",
            "progress": {...}
        }
    ],
    "total": 10
}
```

#### Stream des logs en temps r√©el (SSE)
```http
GET /api/scrape/jobs/{job_id}/logs/stream

Response: Server-Sent Events (SSE)
data: {"timestamp": "...", "level": "info", "message": "Starting scraper..."}
data: {"timestamp": "...", "level": "success", "message": "Added 10 posts"}
```

### 4. Frontend : Interface Utilisateur

#### Composants √† modifier/cr√©er

1. **Bouton "Scrape New Data"** ‚Üí Cr√©e un job et redirige vers la page de monitoring
2. **Page de monitoring des jobs** (`/jobs` ou `/scraping-jobs`)
   - Liste des jobs en cours/termin√©s
   - Barre de progression pour chaque job
   - Logs en temps r√©el (SSE ou polling)
   - Bouton "Cancel" pour chaque job
   - Bouton "Cancel All" global
3. **Notification toast** : "Scraping d√©marr√©, job #1234 cr√©√©"

#### Workflow utilisateur

```
1. Utilisateur clique sur "Scrape New Data"
   ‚Üì
2. Frontend envoie POST /api/scrape/jobs
   ‚Üì
3. Backend cr√©e un job et retourne job_id
   ‚Üì
4. Frontend affiche notification + redirige vers /jobs/{job_id}
   ‚Üì
5. Page de monitoring :
   - Affiche la progression en temps r√©el (polling ou SSE)
   - Affiche les logs en temps r√©el
   - Bouton "Cancel" disponible
   ‚Üì
6. Quand termin√© :
   - Affiche les r√©sultats
   - Bouton "Retour √† la collection" pour voir les nouveaux posts
```

### 5. Impl√©mentation Backend

#### Fichiers √† cr√©er/modifier

**`backend/app/jobs/queue_manager.py`**
```python
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock
import time
from typing import Dict, List, Optional
import uuid

class ScrapingQueueManager:
    def __init__(self, max_workers: int = 3):
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        self.jobs: Dict[str, Dict] = {}
        self.lock = Lock()
    
    def create_job(self, keywords: List[str], sources: List[str], limit: int = 50) -> str:
        job_id = str(uuid.uuid4())
        job = {
            "job_id": job_id,
            "status": "pending",
            "keywords": keywords,
            "sources": sources,
            "limit": limit,
            "created_at": time.time(),
            "progress": {"total": len(keywords) * len(sources), "completed": 0},
            "results": [],
            "errors": [],
            "logs": [],
            "cancelled": False
        }
        with self.lock:
            self.jobs[job_id] = job
        # Sauvegarder dans la DB
        db.create_job_record(job_id)
        # D√©marrer le traitement
        self.executor.submit(self._process_job, job_id)
        return job_id
    
    def _process_job(self, job_id: str):
        # Impl√©mentation du traitement
        pass
    
    def cancel_job(self, job_id: str) -> bool:
        # Annuler un job
        pass
    
    def cancel_all(self) -> int:
        # Annuler tous les jobs
        pass
    
    def get_job(self, job_id: str) -> Optional[Dict]:
        # R√©cup√©rer un job
        pass
```

**`backend/app/routers/scraping_jobs.py`**
```python
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from typing import List

router = APIRouter(prefix="/api/scrape/jobs", tags=["scraping-jobs"])

class JobRequest(BaseModel):
    keywords: List[str]
    sources: List[str] = None
    limit: int = 50
    concurrency: int = 2

@router.post("")
async def create_scraping_job(request: JobRequest):
    # Cr√©er un job
    pass

@router.get("/{job_id}")
async def get_job_status(job_id: str):
    # R√©cup√©rer le statut
    pass

@router.post("/{job_id}/cancel")
async def cancel_job(job_id: str):
    # Annuler un job
    pass

@router.post("/cancel-all")
async def cancel_all_jobs():
    # Annuler tous les jobs
    pass

@router.get("")
async def list_jobs(status: str = None, limit: int = 10):
    # Lister les jobs
    pass
```

### 6. Impl√©mentation Frontend

#### Fichiers √† cr√©er/modifier

**`frontend/jobs/index.html`** (nouvelle page)
- Liste des jobs
- Monitoring en temps r√©el
- Boutons d'annulation

**`frontend/js/jobs.js`** (nouveau fichier)
```javascript
class JobMonitor {
    constructor(jobId) {
        this.jobId = jobId;
        this.eventSource = null;
    }
    
    startMonitoring() {
        // Polling ou SSE pour les logs
        this.pollStatus();
    }
    
    async pollStatus() {
        const response = await fetch(`/api/scrape/jobs/${this.jobId}`);
        const job = await response.json();
        this.updateUI(job);
        if (job.status === 'running') {
            setTimeout(() => this.pollStatus(), 2000);
        }
    }
    
    cancel() {
        fetch(`/api/scrape/jobs/${this.jobId}/cancel`, { method: 'POST' });
    }
}
```

**`frontend/index.html`** (modifier)
- Modifier `scrapeAllSources()` pour cr√©er un job au lieu de lancer directement
- Rediriger vers la page de monitoring

### 7. Logs et Monitoring

#### Logs dans la base de donn√©es
```sql
CREATE TABLE scraping_logs (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    job_id TEXT NOT NULL,
    timestamp REAL NOT NULL,
    level TEXT NOT NULL,  -- info, success, warning, error
    message TEXT NOT NULL,
    source TEXT,
    keyword TEXT
);
```

#### Logs dans les fichiers
- Continuer √† utiliser `log_scraping()` pour les logs fichiers
- Ajouter les logs dans la DB pour l'UI

### 8. Avantages de cette Architecture

1. ‚úÖ **Non-bloquant** : Le frontend n'attend plus les r√©ponses
2. ‚úÖ **Monitoring** : Progression visible en temps r√©el
3. ‚úÖ **Annulation** : Possibilit√© d'annuler √† tout moment
4. ‚úÖ **Persistance** : Les jobs survivent aux red√©marrages
5. ‚úÖ **Scalabilit√©** : Facile d'ajouter plus de workers
6. ‚úÖ **Logs** : Tous les logs sont disponibles dans l'UI
7. ‚úÖ **R√©silience** : Les erreurs n'interrompent pas tout le processus

### 9. Plan d'Impl√©mentation

#### Phase 1 : Backend (Base)
1. Cr√©er `ScrapingQueueManager` avec ThreadPoolExecutor
2. Cr√©er les endpoints `/api/scrape/jobs/*`
3. Int√©grer avec le syst√®me de jobs existant dans `main.py`
4. Ajouter la persistance dans SQLite

#### Phase 2 : Frontend (Base)
1. Modifier `scrapeAllSources()` pour cr√©er un job
2. Cr√©er la page `/jobs` pour le monitoring
3. Impl√©menter le polling pour les mises √† jour
4. Ajouter les boutons d'annulation

#### Phase 3 : Am√©liorations
1. Ajouter SSE pour les logs en temps r√©el
2. Ajouter des statistiques (temps moyen, taux de succ√®s)
3. Ajouter la possibilit√© de relancer un job √©chou√©
4. Ajouter des notifications (email, webhook)

### 10. Migration depuis l'Ancien Syst√®me

- Garder les endpoints `/scrape/*` existants pour compatibilit√©
- Ajouter les nouveaux endpoints `/api/scrape/jobs/*`
- Migrer progressivement le frontend vers le nouveau syst√®me
- Une fois migr√©, d√©pr√©cier les anciens endpoints

## üìù Notes Techniques

- **ThreadPoolExecutor** : Limite le nombre de scrapers simultan√©s (√©vite la surcharge)
- **Polling vs SSE** : Commencer par polling (plus simple), migrer vers SSE si n√©cessaire
- **Timeout** : Ajouter des timeouts sur chaque requ√™te HTTP dans les scrapers
- **Retry** : Impl√©menter un syst√®me de retry pour les √©checs temporaires
- **Rate limiting** : Respecter les limites des APIs externes

## üîÑ Alternatives Consid√©r√©es

1. **WebSockets** : Plus complexe que SSE, pas n√©cessaire pour ce cas d'usage
2. **Celery** : Trop complexe pour commencer, peut √™tre ajout√© plus tard si besoin
3. **Background Tasks FastAPI** : Limit√©, pas de persistance ni de monitoring facile

## ‚úÖ Conclusion

Cette architecture propose une solution progressive :
- **Court terme** : ThreadPoolExecutor + SQLite (simple, fonctionne imm√©diatement)
- **Long terme** : Migration vers Celery si besoin de plus de scalabilit√©

Le syst√®me reste compatible avec l'existant tout en ajoutant les fonctionnalit√©s demand√©es.


# Guide de Test - OVH Complaints Tracker

Ce guide vous explique comment tester le projet √©tape par √©tape.

> **Note:** Ce projet a √©t√© d√©velopp√© **100% avec VibeCoding** (Cursor AI).

## Pr√©requis

- Python 3.11 ou sup√©rieur
- Acc√®s Internet (pour tester les scrapers)

## 1. Configuration de l'environnement

### Cr√©er et activer l'environnement virtuel

```powershell
# Depuis la racine du projet (ovh-complaints-tracker)
cd ovh-complaints-tracker
py -3.11 -m venv .venv

# Activer l'environnement virtuel (PowerShell)
.venv\Scripts\Activate.ps1

# Si vous utilisez CMD
.venv\Scripts\activate.bat
```

### Installer les d√©pendances

```powershell
pip install --upgrade pip
pip install -r backend/requirements.txt
```

## 2. Test de la base de donn√©es

Tester que la base de donn√©es fonctionne correctement :

```powershell
cd backend
python test_db.py
```

**R√©sultat attendu :**
```
Initializing DB...
Inserting sample post...
Rows: [{'id': 1, 'source': 'test', 'author': 'tester', 'content': 'This is a test complaint about OVH domain issues', ...}]
```

## 3. Test du serveur API

### Lancer le serveur FastAPI

Depuis le dossier `backend/` :

```powershell
cd backend
python -m uvicorn app.main:app --reload --port 8000
```

Le serveur devrait d√©marrer et afficher :
```
INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)
INFO:     Started reloader process
INFO:     Started server process
INFO:     Waiting for application startup.
INFO:     Application startup complete.
```

### Acc√©der √† la documentation interactive

Ouvrez votre navigateur et allez √† :
- **Documentation Swagger UI** : http://127.0.0.1:8000/docs
- **Documentation ReDoc** : http://127.0.0.1:8000/redoc

Vous pouvez tester les endpoints directement depuis l'interface Swagger !

## 4. Tester les endpoints API

### 4.1. Endpoint GET /posts

R√©cup√©rer les posts stock√©s dans la base de donn√©es :

**Via le navigateur :**
- http://127.0.0.1:8000/posts?limit=10

**Via PowerShell (Invoke-WebRequest) :**
```powershell
Invoke-WebRequest -Uri "http://127.0.0.1:8000/posts?limit=10" | Select-Object -ExpandProperty Content
```

**Via curl (si disponible) :**
```bash
curl "http://127.0.0.1:8000/posts?limit=10"
```

### 4.2. Endpoint POST /scrape/x

Scraper des tweets de X (Twitter) :

**Via Swagger UI (recommand√©) :**
- Allez sur http://127.0.0.1:8000/docs
- Trouvez l'endpoint `POST /scrape/x`
- Cliquez sur "Try it out"
- Entrez les param√®tres :
  - `query`: "ovh domain" (ou votre recherche)
  - `limit`: 10
- Cliquez sur "Execute"

**Via PowerShell :**
```powershell
Invoke-WebRequest -Uri "http://127.0.0.1:8000/scrape/x?query=ovh%20domain&limit=10" -Method POST
```

**Note :** Le scraper X peut √©chouer si snscrape est bloqu√© par Twitter. C'est normal.

### 4.3. Endpoint POST /scrape/reddit

Scraper des posts Reddit :

**Via Swagger UI :**
- http://127.0.0.1:8000/docs
- Endpoint `POST /scrape/reddit`
- Param√®tres : `query="ovh domain"`, `limit=10`

**Via PowerShell :**
```powershell
Invoke-WebRequest -Uri "http://127.0.0.1:8000/scrape/reddit?query=ovh%20domain&limit=10" -Method POST
```

### 4.4. Endpoint POST /generate-improvement-ideas

G√©n√©rer des id√©es d'am√©lioration produit avec LLM :

**Via Swagger UI :**
- http://127.0.0.1:8000/docs
- Endpoint `POST /generate-improvement-ideas`
- Body: `{"posts": [...], "max_ideas": 5}`

**Via PowerShell :**
```powershell
$body = @{
    posts = @(
        @{
            content = "OVH support is too slow"
            sentiment_label = "negative"
            source = "Twitter"
        }
    )
    max_ideas = 5
} | ConvertTo-Json -Depth 10

Invoke-WebRequest -Uri "http://127.0.0.1:8000/generate-improvement-ideas" `
    -Method POST `
    -ContentType "application/json" `
    -Body $body
```

**Note :** N√©cessite une cl√© API LLM (OVH_API_KEY). Sinon, utilise un fallback bas√© sur des r√®gles.

### 4.5. Endpoint POST /admin/cleanup-duplicates

Nettoyer les doublons dans la base de donn√©es :

```powershell
Invoke-WebRequest -Uri "http://127.0.0.1:8000/admin/cleanup-duplicates" -Method POST
```

**R√©sultat attendu :**
```json
{"deleted": 5, "message": "Successfully removed 5 duplicate posts from database"}
```

## 5. Test du frontend

### Lancer un serveur HTTP simple

Ouvrez un nouveau terminal (gardez le serveur API en cours d'ex√©cution) :

```powershell
cd frontend
python -m http.server 8080
```

### Acc√©der au dashboard

Ouvrez votre navigateur :
- http://localhost:8080

Le dashboard affichera :
- Des donn√©es mock si l'API n'est pas accessible
- Les vrais posts si l'API est accessible (http://127.0.0.1:8000)

### Fonctionnalit√©s √† tester dans le frontend

1. **Filtrage par source** : S√©lectionnez X, Reddit, GitHub, Stack Overflow, etc.
2. **Filtrage par sentiment** : Positive, Negative, Neutral
3. **Filtrage par produit** : Domain, VPS, Hosting, Cloud, etc.
4. **Recherche** : Tapez du texte dans la barre de recherche (recherche dans contenu, auteur, URL)
5. **Timeline & Histogram** : Cliquez sur "üìà View Timeline & Histogram" pour voir les graphiques
6. **Backlog** : Cliquez sur "üìã Backlog" pour ouvrir le panneau lat√©ral
   - Ajoutez des commentaires sous chaque post
   - Basculez entre vue carte et vue liste
   - G√©n√©rez des id√©es d'am√©lioration avec "üí° Generate Ideas"
7. **Boutons de scraping** : 
   - "üÜï Scrape New Data" : Lance tous les scrapers
   - Boutons individuels : Scrape X, Reddit, GitHub, etc.
8. **Preview** : Cliquez sur "üëÅÔ∏è Preview" pour voir le contenu complet d'un post
9. **Light/Dark Mode** : Utilisez le bouton üåì pour changer de th√®me

## 6. Test du scraper X (standalone)

Tester le scraper X ind√©pendamment de l'API :

```powershell
cd backend
python run_scrape_x.py
```

**Note :** Cela peut prendre du temps et peut √©chouer si snscrape est bloqu√©.

## 7. V√©rification de l'√©tat du projet

### Endpoints disponibles

- ‚úÖ `GET /posts` : R√©cup√©rer les posts (fonctionne)
- ‚úÖ `POST /scrape/x` : Scraper X/Twitter (peut √™tre bloqu√© par Twitter)
- ‚úÖ `POST /scrape/reddit` : Scraper Reddit (fonctionne avec limite de taux)
- ‚úÖ `POST /scrape/github` : Scraper GitHub Issues
- ‚úÖ `POST /scrape/stackoverflow` : Scraper Stack Overflow
- ‚úÖ `POST /scrape/trustpilot` : Scraper Trustpilot reviews
- ‚úÖ `POST /generate-improvement-ideas` : G√©n√©rer des id√©es avec LLM
- ‚úÖ `POST /admin/cleanup-duplicates` : Nettoyer les doublons
- ‚úÖ `POST /admin/cleanup-hackernews-posts` : Supprimer les posts Hacker News

### Base de donn√©es

La base de donn√©es PostgreSQL est initialis√©e automatiquement lors du premier d√©marrage de l'API via `app.database.init_db()`.

> **Note :** L'application utilise PostgreSQL depuis janvier 2026 (migration compl√®te depuis DuckDB). Voir [Migration PostgreSQL](../MIGRATION_POSTGRESQL.md) pour plus de d√©tails.

### Fonctionnalit√©s

- ‚úÖ Base de donn√©es DuckDB avec index
- ‚úÖ Analyse de sentiment (VADER)
- ‚úÖ D√©tection automatique de doublons (URL + contenu+auteur+source)
- ‚úÖ Scraper X (Nitter instances)
- ‚úÖ Scraper Reddit (RSS feeds)
- ‚úÖ Scraper GitHub, Stack Overflow, Trustpilot
- ‚úÖ Frontend dashboard avec filtres avanc√©s
- ‚úÖ Backlog sidebar avec commentaires
- ‚úÖ G√©n√©ration d'id√©es avec LLM (OVH AI Endpoints)
- ‚úÖ Timeline & Histogram avec pie chart
- ‚úÖ Product labeling automatique
- ‚úÖ Light/Dark mode
- ‚úÖ Post preview modal

## D√©pannage

### Erreur "Module not found"

Assurez-vous que l'environnement virtuel est activ√© et que les d√©pendances sont install√©es :
```powershell
pip install -r backend/requirements.txt
```

### Erreur de port d√©j√† utilis√©

Si le port 8000 est d√©j√† utilis√©, changez-le :
```powershell
python -m uvicorn app.main:app --reload --port 8001
```
N'oubliez pas de mettre √† jour l'URL dans le frontend (`index.html`) si vous changez le port.

### Le scraper X ne fonctionne pas

C'est normal. Twitter bloque souvent snscrape. Les alternatives :
- Utiliser l'API Twitter officielle (requiert un token)
- Utiliser des donn√©es de test

### Le scraper Reddit ne retourne pas de r√©sultats

- V√©rifiez votre connexion Internet
- Reddit peut limiter les requ√™tes (rate limiting)
- Essayez avec des credentials Reddit (voir README.md)

## 8. Test de la d√©tection de doublons

V√©rifier que les posts dupliqu√©s ne sont pas ins√©r√©s :

```powershell
cd backend
python -c "
from app import db
db.init_db()
# Insert first post
result1 = db.insert_post({
    'source': 'test',
    'author': 'testuser',
    'content': 'Test content',
    'url': 'https://example.com/post1',
    'created_at': '2026-01-13T10:00:00',
    'sentiment_score': -0.5,
    'sentiment_label': 'negative'
})
print(f'First insert: {result1}')  # Should be True

# Try to insert same URL again
result2 = db.insert_post({
    'source': 'test',
    'author': 'testuser2',
    'content': 'Different content',
    'url': 'https://example.com/post1',  # Same URL
    'created_at': '2026-01-13T11:00:00',
    'sentiment_score': -0.3,
    'sentiment_label': 'negative'
})
print(f'Duplicate insert: {result2}')  # Should be False
"
```

**R√©sultat attendu :**
```
First insert: True
Duplicate insert: False
```

## 9. Test des jobs en arri√®re-plan

Tester le syst√®me de jobs pour le scraping de keywords multiples :

### 9.1. D√©marrer un job

```powershell
$body = @{
    keywords = @("OVH", "domain")
} | ConvertTo-Json

Invoke-WebRequest -Uri "http://127.0.0.1:8000/scrape/keywords?limit=10&concurrency=2&delay=0.5" `
    -Method POST `
    -ContentType "application/json" `
    -Body $body
```

**R√©sultat attendu :**
```json
{"job_id": "uuid-123-456-789"}
```

### 9.2. V√©rifier le statut du job

```powershell
$jobId = "uuid-123-456-789"  # Remplacer par l'ID retourn√©
Invoke-WebRequest -Uri "http://127.0.0.1:8000/scrape/jobs/$jobId" | Select-Object -ExpandProperty Content
```

**R√©sultat attendu :**
```json
{
  "id": "uuid-123-456-789",
  "status": "running",
  "progress": {"total": 12, "completed": 5},
  "results": [{"added": 3}, {"added": 2}],
  "errors": []
}
```

### 9.3. Annuler un job

```powershell
Invoke-WebRequest -Uri "http://127.0.0.1:8000/scrape/jobs/$jobId/cancel" -Method POST
```

## 10. Test des index de base de donn√©es

V√©rifier que les index sont cr√©√©s correctement :

```powershell
cd backend
python -c "
from app.db import get_db_connection

conn, is_duckdb = get_db_connection()
c = conn.cursor()
c.execute(\"SELECT indexname FROM pg_indexes WHERE tablename='posts'\")
indexes = c.fetchall()
print('Indexes on posts table:')
for idx in indexes:
    print(f'  - {idx[0]}')
conn.close()
"
```

> **Note :** L'application utilise PostgreSQL. La syntaxe pour v√©rifier les index utilise `pg_indexes`.

**R√©sultat attendu :**
```
Indexes on posts table:
  - idx_posts_source
  - idx_posts_sentiment_label
  - idx_posts_created_at
  - idx_posts_language
  - idx_posts_url
```

## 11. Test de performance

Tester les performances avec un grand nombre de posts :

```powershell
cd backend
python -c "
import time
from app import db

db.init_db()

# Insert 1000 test posts
start = time.time()
for i in range(1000):
    db.insert_post({
        'source': 'test',
        'author': f'user{i}',
        'content': f'Test content {i}',
        'url': f'https://example.com/post{i}',
        'created_at': '2026-01-13T10:00:00',
        'sentiment_score': -0.5,
        'sentiment_label': 'negative'
    })

insert_time = time.time() - start
print(f'Inserted 1000 posts in {insert_time:.2f}s')

# Test query performance
start = time.time()
posts = db.get_posts(limit=100, offset=0, language=None)
query_time = time.time() - start
print(f'Queried 100 posts in {query_time:.4f}s')
print(f'Average: {query_time/100*1000:.2f}ms per post')
"
```

**R√©sultat attendu :**
- Insertion : < 5 secondes pour 1000 posts
- Requ√™te : < 0.1 seconde pour 100 posts

## 12. Test d'int√©gration complet

Sc√©nario de test end-to-end :

```powershell
# 1. D√©marrer le serveur (dans un terminal)
cd backend
python -m uvicorn app.main:app --reload --port 8000

# 2. Dans un autre terminal, ex√©cuter le test
cd backend
python -c "
import httpx
import time

API_BASE = 'http://127.0.0.1:8000'

# Test 1: Scraper Trustpilot
print('1. Testing Trustpilot scraper...')
r = httpx.post(f'{API_BASE}/scrape/trustpilot?limit=5', timeout=30)
print(f'   Status: {r.status_code}, Added: {r.json()[\"added\"]}')

# Test 2: V√©rifier les posts
print('2. Checking posts...')
r = httpx.get(f'{API_BASE}/posts?limit=10')
posts = r.json()
print(f'   Found {len(posts)} posts')

# Test 3: D√©marrer un job
print('3. Starting background job...')
r = httpx.post(f'{API_BASE}/scrape/keywords?limit=5&concurrency=1&delay=0.1',
               json={'keywords': ['OVH']}, timeout=5)
job_id = r.json()['job_id']
print(f'   Job ID: {job_id}')

# Test 4: V√©rifier le statut du job
print('4. Checking job status...')
time.sleep(2)
r = httpx.get(f'{API_BASE}/scrape/jobs/{job_id}')
status = r.json()
print(f'   Status: {status[\"status\"]}, Progress: {status[\"progress\"]}')

print('\\n‚úÖ Integration test completed!')
"
```

## Prochaines √©tapes

Apr√®s avoir test√© le projet, vous pouvez :
1. Am√©liorer les scrapers
2. Ajouter de nouvelles fonctionnalit√©s
3. Am√©liorer le frontend
4. Ajouter des tests unitaires
5. Tester les performances avec de plus gros volumes de donn√©es
6. V√©rifier la d√©tection de doublons avec des donn√©es r√©elles



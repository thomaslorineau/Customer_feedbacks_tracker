# Audit des Fonctions de Scraping - OVH Complaints Tracker

**Date**: 2026-01-13  
**Version**: 1.0

## R√©sum√© Ex√©cutif

Cet audit identifie les probl√®mes et limitations de chaque scraper dans le syst√®me. Sur **11 scrapers** analys√©s, plusieurs pr√©sentent des probl√®mes critiques ou des limitations qui peuvent emp√™cher leur fonctionnement correct.

---

## 1. Scraper X/Twitter (`x_scraper.py`)

### ‚úÖ Points Positifs
- Strat√©gie de fallback avec plusieurs instances Nitter
- Gestion d'erreurs robuste
- D√©tection de langue

### ‚ö†Ô∏è Probl√®mes Identifi√©s

1. **D√©pendance √† Nitter (CRITIQUE)**
   - Les instances Nitter sont souvent instables ou bloqu√©es
   - Pas de m√©canisme de v√©rification de disponibilit√©
   - Si toutes les instances √©chouent, retourne une liste vide sans erreur

2. **API Twitter non fonctionnelle**
   - Ligne 135-149 : L'API Twitter est comment√©e car elle n√©cessite un Bearer token
   - Aucune alternative r√©elle si Nitter √©choue

3. **Parsing HTML fragile**
   - D√©pend de la structure HTML de Nitter qui peut changer
   - S√©lecteurs CSS potentiellement obsol√®tes (lignes 87-102)

### üîß Recommandations
- Ajouter une v√©rification de sant√© des instances Nitter avant utilisation
- Impl√©menter un syst√®me de rotation d'instances
- Ajouter un m√©canisme de notification si toutes les m√©thodes √©chouent
- Documenter que ce scraper peut retourner 0 r√©sultats sans erreur

**Statut**: ‚ö†Ô∏è **FONCTIONNEL MAIS INSTABLE**

---

## 2. Scraper Reddit (`reddit.py`)

### ‚úÖ Points Positifs
- Utilise les flux RSS officiels de Reddit (m√©thode l√©gale)
- Retry logic avec exponential backoff
- Gestion des erreurs r√©seau robuste
- User-Agent appropri√©

### ‚ö†Ô∏è Probl√®mes Identifi√©s

1. **Limitation des flux RSS**
   - Reddit limite les r√©sultats RSS (max 100 items)
   - Les flux RSS peuvent √™tre incomplets ou filtr√©s

2. **Rate Limiting**
   - Reddit peut bloquer les requ√™tes r√©p√©t√©es
   - Pas de gestion explicite du rate limiting (429)

3. **Parsing des dates**
   - D√©pend de `published_parsed` qui peut √™tre absent
   - Fallback sur `datetime.now()` peut cr√©er des dates incorrectes

### üîß Recommandations
- Ajouter une gestion explicite du code 429 (Too Many Requests)
- Impl√©menter un cache pour √©viter les requ√™tes r√©p√©t√©es
- Am√©liorer le parsing des dates avec plusieurs formats

**Statut**: ‚úÖ **FONCTIONNEL** (avec limitations connues)

---

## 3. Scraper GitHub (`github.py`)

### ‚úÖ Points Positifs
- Utilise l'API GitHub officielle (gratuite, pas d'auth requise)
- Recherche √† la fois Issues et Discussions
- Timeout configur√©

### ‚ö†Ô∏è Probl√®mes Identifi√©s

1. **Rate Limiting GitHub (CRITIQUE)**
   - GitHub limite √† 60 requ√™tes/heure pour les requ√™tes non authentifi√©es
   - Aucune gestion du rate limiting (code 403)
   - Pas de retry avec backoff pour les erreurs de rate limit

2. **Limitation de r√©sultats**
   - L'API GitHub limite √† 100 r√©sultats par requ√™te
   - Pas de pagination pour r√©cup√©rer plus de r√©sultats

3. **Erreurs silencieuses**
   - Si l'API √©choue, retourne une liste vide sans log d'erreur d√©taill√©
   - Les exceptions sont catch√©es mais pas toujours logg√©es

### üîß Recommandations
- Ajouter une gestion du rate limiting avec retry
- Impl√©menter la pagination pour plus de r√©sultats
- Ajouter des logs d'erreur plus d√©taill√©s
- Consid√©rer l'utilisation d'un token GitHub (optionnel) pour augmenter les limites

**Statut**: ‚ö†Ô∏è **FONCTIONNEL MAIS LIMIT√â PAR RATE LIMITING**

---

## 4. Scraper Stack Overflow (`stackoverflow.py`)

### ‚úÖ Points Positifs
- Utilise l'API Stack Exchange officielle (gratuite, bien document√©e)
- Retry logic avec exponential backoff
- Gestion des timeouts

### ‚ö†Ô∏è Probl√®mes Identifi√©s

1. **Rate Limiting Stack Exchange**
   - Stack Exchange limite √† 300 requ√™tes/jour par IP
   - Pas de gestion explicite du quota
   - Peut √©chouer silencieusement si quota d√©pass√©

2. **Filtre de recherche limit√©**
   - Utilise `intitle` qui ne recherche que dans les titres
   - Peut manquer des questions pertinentes dans le corps

3. **Parsing des dates**
   - Conversion timestamp peut √©chouer si `creation_date` est absent
   - Fallback sur `datetime.now()` peut √™tre incorrect

### üîß Recommandations
- Ajouter une v√©rification du quota API avant requ√™te
- Am√©liorer la requ√™te de recherche pour inclure le corps
- Am√©liorer le parsing des dates avec validation

**Statut**: ‚úÖ **FONCTIONNEL** (avec limitations de quota)

---

## 5. Scraper Trustpilot (`trustpilot.py`)

### ‚úÖ Points Positifs
- Strat√©gie multi-niveaux (HTML ‚Üí API ‚Üí Sample)
- Parsing HTML robuste avec BeautifulSoup
- Mapping rating ‚Üí sentiment automatique

### ‚ö†Ô∏è Probl√®mes Identifi√©s

1. **Fallback vers donn√©es sample (CRITIQUE)**
   - Ligne 56 : Si HTML et API √©chouent, retourne des donn√©es sample
   - **VIOLATION** : Le syst√®me ne devrait jamais retourner de donn√©es sample
   - Les donn√©es sample polluent la base de donn√©es

2. **Parsing HTML fragile**
   - D√©pend de s√©lecteurs CSS sp√©cifiques (lignes 78-110)
   - Structure HTML de Trustpilot peut changer
   - Peut √©chouer silencieusement si structure change

3. **API n√©cessite une cl√©**
   - L'API Trustpilot n√©cessite une cl√© API (non fournie)
   - Pas de fallback gracieux si cl√© manquante

4. **URL hardcod√©e**
   - Ligne 15 : URL hardcod√©e vers `fr.trustpilot.com`
   - Ne supporte pas d'autres langues/r√©gions

### üîß Recommandations
- **URGENT** : Supprimer le fallback vers donn√©es sample (ligne 56)
- Am√©liorer la robustesse du parsing HTML
- Ajouter une documentation pour obtenir une cl√© API Trustpilot
- Rendre l'URL configurable

**Statut**: ‚ùå **PROBL√àME CRITIQUE** (retourne des donn√©es sample)

---

## 6. Scraper Google News (`news.py`)

### ‚úÖ Points Positifs
- Utilise les flux RSS de Google News (m√©thode l√©gale)
- Gestion de plusieurs keywords
- Retry logic avec exponential backoff
- √âvite les doublons avec `seen_urls`

### ‚ö†Ô∏è Probl√®mes Identifi√©s

1. **Limitation des flux RSS Google**
   - Google News RSS peut √™tre limit√© ou filtr√©
   - Pas de garantie de r√©sultats complets

2. **Parsing des dates**
   - D√©pend de `published_parsed` qui peut √™tre absent
   - Fallback sur `datetime.now()` peut √™tre incorrect

3. **Extraction d'auteur fragile**
   - Lignes 98-105 : Logique complexe pour extraire l'auteur
   - Peut √©chouer si structure RSS change

### üîß Recommandations
- Am√©liorer le parsing des dates avec validation
- Simplifier l'extraction d'auteur
- Ajouter des logs pour debug

**Statut**: ‚úÖ **FONCTIONNEL** (avec limitations connues)

---

## 7. Scraper OVH Forum (`ovh_forum.py`)

### ‚úÖ Points Positifs
- Utilise des techniques anti-bot (stealth session, headers r√©alistes)
- Timeout global pour √©viter les boucles infinies
- Fallback vers browser automation (Selenium/Playwright)

### ‚ö†Ô∏è Probl√®mes Identifi√©s

1. **Scraping HTML fragile (CRITIQUE)**
   - D√©pend fortement de la structure HTML du forum
   - S√©lecteurs CSS peuvent √™tre obsol√®tes (lignes 110-121)
   - Peut retourner 0 r√©sultats si structure change

2. **Performance lente**
   - Fait des requ√™tes individuelles pour chaque post (ligne 173)
   - Peut prendre tr√®s longtemps avec beaucoup de posts
   - Timeout de 60s peut √™tre insuffisant

3. **Browser automation optionnel**
   - N√©cessite Playwright ou Selenium install√©s
   - Peut √©chouer si non disponible

4. **Filtrage par query limit√©**
   - Ligne 157 : Filtre simple par substring dans titre/URL
   - Peut manquer des posts pertinents

### üîß Recommandations
- Am√©liorer la robustesse des s√©lecteurs CSS
- Optimiser pour r√©duire le nombre de requ√™tes
- Documenter les d√©pendances optionnelles (Playwright/Selenium)
- Am√©liorer le filtrage par query

**Statut**: ‚ö†Ô∏è **INSTABLE** (d√©pend de la structure HTML)

---

## 8. Scraper Mastodon (`mastodon.py`)

### ‚úÖ Points Positifs
- Essaie plusieurs instances Mastodon
- Utilise l'API publique Mastodon
- √âvite les doublons avec `seen_urls`

### ‚ö†Ô∏è Probl√®mes Identifi√©s

1. **Recherche par hashtag limit√©e**
   - Ligne 58 : Convertit la query en hashtag (supprime espaces)
   - Peut ne pas trouver de r√©sultats si query n'est pas un hashtag populaire

2. **API de recherche peut √™tre limit√©e**
   - Certaines instances Mastodon limitent l'API de recherche
   - Pas de gestion explicite des erreurs 429

3. **Parsing HTML dans contenu**
   - Ligne 74 : Supprime les balises HTML mais peut laisser du contenu mal format√©

### üîß Recommandations
- Am√©liorer la strat√©gie de recherche (hashtag + texte)
- Ajouter une gestion du rate limiting
- Am√©liorer le nettoyage HTML

**Statut**: ‚úÖ **FONCTIONNEL** (avec limitations)

---

## 9. Scraper G2 Crowd (`g2_crowd.py`)

### ‚úÖ Points Positifs
- Utilise des techniques anti-bot
- Timeout global pour √©viter les boucles infinies
- Fallback vers browser automation

### ‚ö†Ô∏è Probl√®mes Identifi√©s

1. **Scraping HTML tr√®s fragile (CRITIQUE)**
   - D√©pend fortement de la structure HTML de G2
   - S√©lecteurs CSS multiples et complexes (lignes 90-99)
   - Peut facilement √©chouer si structure change

2. **Blocage fr√©quent (403)**
   - G2 bloque souvent les scrapers (ligne 66)
   - N√©cessite browser automation qui peut ne pas √™tre disponible

3. **Performance lente**
   - Limite √† 15 reviews max (ligne 104)
   - Peut prendre du temps m√™me avec peu de r√©sultats

4. **URL hardcod√©e**
   - Ligne 42 : URL hardcod√©e vers `/products/ovhcloud/reviews`
   - Ne supporte pas d'autres produits

### üîß Recommandations
- **URGENT** : Am√©liorer la robustesse du parsing HTML
- Documenter les d√©pendances optionnelles
- Rendre l'URL configurable
- Consid√©rer l'utilisation de l'API G2 si disponible

**Statut**: ‚ùå **TR√àS INSTABLE** (bloqu√© fr√©quemment)

---

## 10. Endpoint `/scrape/github` dans `main.py`

### ‚ö†Ô∏è Probl√®me Critique Identifi√©

**Ligne 758** : Le scraper GitHub **ne v√©rifie pas** si `insert_post` a r√©ussi !

```python
db.insert_post({...})
added += 1  # Toujours incr√©ment√©, m√™me si insert a √©chou√© !
```

Cela peut cr√©er des compteurs incorrects et masquer des erreurs de base de donn√©es.

### üîß Recommandation
- **URGENT** : V√©rifier le retour de `insert_post` avant d'incr√©menter `added`

**Statut**: ‚ùå **BUG CRITIQUE**

---

## 11. Endpoint `/scrape/stackoverflow` dans `main.py`

### ‚ö†Ô∏è Probl√®me Identifi√©

**Ligne 742** : M√™me probl√®me - ne v√©rifie pas le retour de `insert_post` correctement, mais au moins utilise `if db.insert_post(...)`.

**Statut**: ‚úÖ **CORRECT** (mais similaire au pattern de GitHub)

---

## 12. Endpoint `/scrape/news` dans `main.py`

### ‚ö†Ô∏è Probl√®me Identifi√©

**Ligne 903** : Le param√®tre `query` est **obligatoire** mais n'a pas de valeur par d√©faut.

Si appel√© sans `query`, FastAPI retournera une erreur 422.

**Statut**: ‚ö†Ô∏è **INCOH√âRENT** (devrait avoir une valeur par d√©faut comme les autres)

---

## Probl√®mes G√©n√©raux

### 1. Gestion des Erreurs Incoh√©rente
- Certains scrapers retournent `[]` en cas d'erreur
- D'autres l√®vent des exceptions
- Pas de standardisation

### 2. Logging Incoh√©rent
- Certains utilisent `logger.info()`, d'autres `print()`
- Niveaux de log diff√©rents
- Pas de format standardis√©

### 3. Donn√©es Sample
- Trustpilot retourne des donn√©es sample (VIOLATION)
- Doit √™tre supprim√© imm√©diatement

### 4. Rate Limiting
- Peu de scrapers g√®rent explicitement le rate limiting
- Peut causer des blocages IP

### 5. Timeouts
- Timeouts diff√©rents selon les scrapers
- Pas de configuration centralis√©e

---

## Recommandations Globales

### Priorit√© Haute (URGENT)

1. **Supprimer les donn√©es sample de Trustpilot** (ligne 56 de `trustpilot.py`)
2. **Corriger le bug GitHub** (ligne 758 de `main.py`)
3. **Ajouter une valeur par d√©faut pour `query` dans `/scrape/news`**

### Priorit√© Moyenne

1. Standardiser la gestion des erreurs (tous retournent `[]` ou tous l√®vent des exceptions)
2. Standardiser le logging (utiliser `logger` partout)
3. Ajouter une gestion centralis√©e du rate limiting
4. Am√©liorer la robustesse des scrapers HTML (OVH Forum, G2 Crowd)

### Priorit√© Basse

1. Ajouter des tests unitaires pour chaque scraper
2. Documenter les limitations de chaque scraper
3. Cr√©er un syst√®me de monitoring de sant√© des scrapers

---

## Tests Recommand√©s

Pour chaque scraper, tester :
1. ‚úÖ Scraping avec query valide
2. ‚úÖ Scraping avec query vide/invalide
3. ‚úÖ Gestion des erreurs r√©seau
4. ‚úÖ Gestion du rate limiting
5. ‚úÖ Parsing des donn√©es retourn√©es
6. ‚úÖ Insertion en base de donn√©es

---

## Conclusion

Sur **11 scrapers** analys√©s :
- ‚úÖ **4 fonctionnels** : Reddit, Stack Overflow, Google News, Mastodon
- ‚ö†Ô∏è **5 instables/limites** : X/Twitter, GitHub, OVH Forum, News (query), Trustpilot
- ‚ùå **2 critiques** : Trustpilot (donn√©es sample), GitHub (bug insertion)

**Taux de fonctionnalit√© estim√©** : ~60% des scrapers fonctionnent correctement dans des conditions normales.

---

## Corrections Appliqu√©es

### ‚úÖ Probl√®mes Critiques Corrig√©s

1. **Trustpilot - Suppression des donn√©es sample** ‚úÖ
   - **Fichier** : `backend/app/scraper/trustpilot.py`
   - **Ligne 56** : Supprim√© le fallback vers `_get_sample_trustpilot_reviews()`
   - **R√©sultat** : Retourne maintenant une liste vide si tous les scrapers √©chouent

2. **GitHub - Correction du bug d'insertion** ‚úÖ
   - **Fichier** : `backend/app/main.py`
   - **Ligne 758** : Ajout√© une v√©rification du retour de `insert_post()` avant d'incr√©menter `added`
   - **R√©sultat** : Le compteur `added` refl√®te maintenant correctement le nombre de posts r√©ellement ins√©r√©s

3. **News - Ajout de valeur par d√©faut pour query** ‚úÖ
   - **Fichier** : `backend/app/main.py`
   - **Ligne 903** : Ajout√© `query: str = "OVH"` comme valeur par d√©faut
   - **R√©sultat** : L'endpoint peut maintenant √™tre appel√© sans param√®tre `query`

### üìã Probl√®mes Restants (Priorit√© Moyenne/Basse)

- Am√©liorer la robustesse des scrapers HTML (OVH Forum, G2 Crowd)
- Standardiser la gestion des erreurs
- Ajouter une gestion centralis√©e du rate limiting
- Am√©liorer le logging

